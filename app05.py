import streamlit as st, time, json,pandas as pd
from accessments import form_base,form_cognitive,form_emotion,form_note,form_physic
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

st.title('ğŸ¤– AIè­·ç†è¨˜éŒ„ç”Ÿæˆç³»çµ±05-JSONç¯‡')
#å»ºç«‹session_state
if "t1Text" not in st.session_state: st.session_state.t1Text={}
if "t2Text" not in st.session_state: st.session_state.t2Text='ğŸ€ JSONç·¨è¼¯'
if "t3Text" not in st.session_state: st.session_state.t3Text='ğŸ’ AIç”Ÿæˆ'
if "t4Text" not in st.session_state: st.session_state.t4Text='ğŸ“Œ è³‡æ–™ç®¡ç†'
if "t5Text" not in st.session_state: st.session_state.t5Text='ğŸš© é¢¨éšªè©•ä¼°'
if "model" not in st.session_state: st.session_state.model='mistral:latest'
@st.cache_resource 
def AI_generate(text):
    llmModel=ChatOllama(model=st.session_state.model)
    myParser=StrOutputParser()
    myPrompt=ChatPromptTemplate(messages=[
        ('system','æ ¹åŠ‡æä¾›è³‡æ–™ç”Ÿæˆä¸€ä»½æè¿°æ€§çš„è­·ç†è¨˜éŒ„,ä¸è¦ä½¿ç”¨è¡¨åˆ—,è«‹ä½¿ç”¨å®Œæ•´çš„å¥å­æè¿°,é™å®šä½¿ç”¨ç¹é«”ä¸­æ–‡å’Œå°ç£ç”¨èªã€‚'),
        ('user',text)]
        )
    myChain=myPrompt|llmModel|myParser
    beginTime=time.time()
    myResponse=myChain.invoke({'user':text})
    return myResponse+'(è€—æ™‚'+str(round(time.time()-beginTime,2))+'ç§’)'

#å»ºç«‹tabs
t1,t2,t3,t4,t5=st.tabs(['ğŸ® å¿«é€Ÿé¸å–®','ğŸ€ JSONç·¨è¼¯','ğŸ’ AIç”Ÿæˆ','ğŸ“Œ AIå°è©±','ğŸš© é¢¨éšªè©•ä¼°'])
with t1:
    myDict={}
    myDict.update(form_base())
    myDict.update(form_cognitive())
    myDict.update(form_emotion())
    myDict.update(form_physic())
    myDict.update(form_note())
    st.session_state.t1Text.update(myDict)  
    st.session_state.t2Text=st.session_state.t1Text
with t2:
    t2TextArea=st.text_area('å…¥JSONæ ¼å¼è³‡æ–™',value=st.session_state.t2Text)
    t2Button=st.button('å„²å­˜æ­¤JSONæ ¼å¼è³‡æ–™')
    if t2Button:
        st.session_state.t2Text=t2TextArea
        # with open('myPatient.json','a+',encoding='utf-8') as f:
        #     f.write(json.dumps(st.session_state.t2Text,ensure_ascii=False))
        #     f.write(',')
        #     f.close()
        st.session_state.t3Text=st.session_state.t2Text
        st.success('å„²å­˜æˆåŠŸ')
    #st.write(st.session_state.t2Text)
with t3: ## AIç”Ÿæˆ
    with st.sidebar.container():
        values=['mistral:latest','gemma2:latest','deepseek-r1:8b','phi4:latest','lamma3.2:latest','mistral-small:24b']
        default_index=values.index('mistral:latest')
        model=st.selectbox('é¸æ“‡æ¨¡å‹',values,index=default_index)
        st.session_state.model=model
    myString=st.session_state.t3Text.replace('{','').replace('}','').replace('\"','').replace(':','').replace(',','').replace('\'','')
    myString=AI_generate(myString)
    st.write(myString)
   
    t3Button=st.button('ç¢“å®šä¿®æ”¹')
    if t3Button:
        t3TextArea=st.text_area('AIç”Ÿæˆ',value=AI_generate(myString),height=400)
        st.session_state.t3Text=t3TextArea
        st.session_state.t4Text=t3TextArea
with t4: ## AIå°è©±
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferWindowMemory
    myMemory=ConversationBufferWindowMemory(k=7)
    myConversation=ConversationChain(memory=myMemory,llm=ChatOllama(model=st.session_state.model),verbose=False)
    systemPrompt=(
        "é‡å°ä»¥ä¸‹çš„è­·ç†è¨˜éŒ„é€²è¡Œå°è©±ï¼Œè«‹ä¾ç…§ä½¿ç”¨è€…çš„å•é¡Œé€²è¡Œä¿®æ”¹ã€‚"
        "å…§å®¹å¿…éœ€ç¬¦åˆè­·ç†è¨˜éŒ„çš„äº‹å¯¦å…§å®¹ï¼Œä¸¦ä¸”ä½¿ç”¨ç¹é«”ä¸­æ–‡å’Œå°ç£ç”¨èªã€‚"
        "ä¸å¯ä»¥ä½¿ç”¨è¡¨åˆ—ï¼Œè«‹ä½¿ç”¨å®Œæ•´çš„å¥å­æè¿°ã€‚"
        "ä¸çŸ¥é“çš„å…§å®¹å¯ä»¥è©¢å•,ä½†ä¸å¯ä»¥éš¨æ„ç·¨é€ å…§å®¹ã€‚"
        "å…§å®¹å°‘æ–¼1000å­—å…ƒ:\n\n`"
        f"{st.session_state.t4Text}"
    )
    print(systemPrompt)
    myConversation.memory.save_context({'role':'system'},{'content':systemPrompt})
    with st.chat_message('ai'):
        st.write(f'ğŸ¤– AI: {systemPrompt}')
    # for message in myMemory:
    #     with st.chat_message(message.role):
    #         st.write(f'{message.role}: {message.content}')

    userPrompt=st.chat_input('è«‹å¹«æˆ‘æ‘˜è¦æˆ500å€‹å­—å…ƒ')
    if userPrompt:
        myConversation.memory.save_context({'role':'user'},{'content':userPrompt})
        beginTime=time.time()
        myResponse=myConversation.invoke(userPrompt)
        endTime=time.time() 
        st.write(f'ğŸ‘©â€âš•ï¸ ä½ : {userPrompt}')
        st.write(f'ğŸ¤– AI: {myResponse} è€—æ™‚: {round(endTime-beginTime,2)}ç§’')
with t5:
    from langchain_community.document_loaders import PyPDFLoader 
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.embeddings import OllamaEmbeddings
    from langchain_community.vectorstores import Chroma
    myLlm=ChatOllama(model=st.session_state.model)  
    myLoader=PyPDFLoader('é é˜²ç—…äººè·Œå€’.pdf')###,encoding='utf-8')
    page=myLoader.load_page(0)
    text_splitter=RecursiveCharacterTextSplitter(
        chunk_size=100,
        chunk_overlap=10,
        separators=['ã€‚','?','!','\n']
    )
    docs=[]
    for page in page:
        splits=text_splitter.split(page.page_content)
        for chunk in splits:
            docs.append(chunk)
    embeddings=OllamaEmbeddings(llm=myLlm)
    db=Chroma.from_texts(texts=docs,embeddings=embeddings,persist_directory='db_chroma')
    st.session_state.t5Text=f'å…±æœ‰{len(docs)}ç­†è³‡æ–™'
    st.write(st.session_state.t5Text)
    st.write('é¢¨éšªè©•ä¼°')
    st.write('é¢¨éšªè©•ä¼°')